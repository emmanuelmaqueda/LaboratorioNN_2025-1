{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "El algoritmo del Gradiente Descendente convencional (también conocido como batch gradient descent o Gradiente Descendente por lotes) usa la totalidad de los ejemplos de entrenamiento en cada iteración del entrenamiento.\n",
        "\n",
        "La principal desventaja de este enfoque es que si tenemos demasiados ejemplos de entrenamiento (como ocurre usualmente en el Deep Learning), cada iteración de entrenamiento requerirá demasiado tiempo y además se amplían los requisitos de memoria RAM del equipo usado en esta fase de desarrollo.\n",
        "\n",
        "El algoritmo del Gradiente Descendente Estocástico es precisamente una alternativa a este inconveniente."
      ],
      "metadata": {
        "id": "kqUbKbv8-HFM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##El algoritmo del Gradiente Descendente Estocástico\n",
        "Es el extremo opuesto al Gradiente Descendente convencional: en lugar de tomar todo el set de entrenamiento en cada iteración, el Gradiente Descendente Estocástico toma tan sólo un ejemplo de entrenamiento de forma totalmente aleatoria (de allí el término “estocástico”).\n",
        "\n",
        "La ventaja de esto es que se requieren menos recursos de memoria y el entrenamiento es mucho más rápido. Sin embargo, la desventaja es que al usar tan sólo un ejemplo de entrenamiento en cada iteración el algoritmo se puede quedar estancado en un mínimo local.\n",
        "\n",
        "El algoritmo del Gradiente Descendente Estocástico con mini-batch (o mini-lotes), es un punto intermedio entre estos dos extremos.\n",
        "\n",
        "##Mini-batch gradient descent\n",
        "\n",
        "En este caso en cada iteración no se toma ni todo el set de entrenamiento ni tan sólo un ejemplo, sino que se toman bloques de datos provenientes de dicho set. Cada uno de estos bloques se conoce como “lote” (o batch en Inglés) y usualmente puede tener entre unas cuantas decenas y unos cuantos miles de ejemplos (todo depende del tamaño del set de entrenamiento).\n",
        "\n",
        "Así, en cada iteración de entrenamiento se presentan al modelo n lotes a partir de los cuales se actualizan sus parámetros.\n",
        "\n",
        "La ventaja de esto es que se disminuye la probabilidad de estancamiento en un mínimo local (como ocurría con el Gradiente Descendente Estocástico) y además el entrenamiento se realiza más rápido y requiere menos recursos de memoria (en comparación con el Gradiente Descendente convencional).\n",
        "\n"
      ],
      "metadata": {
        "id": "ehevtbDt-PNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generar datos de ejemplo\n",
        "np.random.seed(0)\n",
        "X = 2 * np.random.rand(100, 1)\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)\n",
        "X_b = np.c_[np.ones((100, 1)), X]  # Añadir el término bias (intercepto)\n",
        "learning_rate = 0.1\n",
        "n_iterations = 1000\n",
        "theta = np.random.randn(2, 1)\n",
        "\n",
        "# Batch Gradient Descent\n",
        "def batch_gradient_descent(X, y, theta, learning_rate, n_iterations):\n",
        "    m = len(y)\n",
        "    for iteration in range(n_iterations):\n",
        "        gradients = (2/m) * X.T.dot(X.dot(theta) - y)\n",
        "        theta = theta - learning_rate * gradients\n",
        "    return theta\n",
        "\n",
        "\n",
        "# Función para medir el tiempo y entrenar el modelo\n",
        "def medir_tiempo(entrenar_funcion, X, y, theta, learning_rate, n_iterations, batch_size=None):\n",
        "    start_time = time.time()  # Inicia el cronómetro\n",
        "    if batch_size:\n",
        "        theta_final = entrenar_funcion(X, y, theta, learning_rate, n_iterations, batch_size)\n",
        "    else:\n",
        "        theta_final = entrenar_funcion(X, y, theta, learning_rate, n_iterations)\n",
        "    end_time = time.time()  # Finaliza el cronómetro\n",
        "    tiempo_total = end_time - start_time\n",
        "    return theta_final, tiempo_total"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrPz3V-8-vme",
        "outputId": "43d282a4-8143-40c9-cae7-3c408e4b6789"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tiempo de entrenamiento (Batch): 0.0311 segundos\n",
            "Tiempo de entrenamiento (SGD): 1.5444 segundos\n",
            "Tiempo de entrenamiento (Mini-Batch): 0.0460 segundos\n",
            "Error MSE (Batch): 0.9924\n",
            "Error MSE (SGD): 1.1372\n",
            "Error MSE (Mini-Batch): 1.0004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4SpbWVEWAGPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Stochastic Gradient Descent\n",
        "def stochastic_gradient_descent(X, y, theta, learning_rate, n_iterations):"
      ],
      "metadata": {
        "id": "Kpkc9UBN__pT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Mini-Batch Gradient Descent\n",
        "def mini_batch_gradient_descent(X, y, theta, learning_rate, n_iterations, batch_size):"
      ],
      "metadata": {
        "id": "tIFnRufeAHW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Medir tiempo para Batch Gradient Descent\n",
        "theta_batch, tiempo_batch = medir_tiempo(batch_gradient_descent, X_b, y, theta, learning_rate, n_iterations)\n",
        "print(f\"Tiempo de entrenamiento (Batch): {tiempo_batch:.4f} segundos\")\n",
        "\n",
        "# Medir tiempo para Stochastic Gradient Descent\n",
        "theta_sgd, tiempo_sgd = medir_tiempo(stochastic_gradient_descent, X_b, y, theta, learning_rate, n_iterations)\n",
        "print(f\"Tiempo de entrenamiento (SGD): {tiempo_sgd:.4f} segundos\")\n",
        "\n",
        "# Medir tiempo para Mini-Batch Gradient Descent\n",
        "theta_mini_batch, tiempo_mini_batch = medir_tiempo(mini_batch_gradient_descent, X_b, y, theta, learning_rate, n_iterations, batch_size=20)\n",
        "print(f\"Tiempo de entrenamiento (Mini-Batch): {tiempo_mini_batch:.4f} segundos\")\n"
      ],
      "metadata": {
        "id": "Yoe2WtHx_6-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Calcular predicciones\n",
        "y_pred_batch = X_b.dot(theta_batch)\n",
        "y_pred_sgd = X_b.dot(theta_sgd)\n",
        "y_pred_mini_batch = X_b.dot(theta_mini_batch)\n",
        "\n",
        "# Calcular errores\n",
        "mse_batch = mean_squared_error(y, y_pred_batch)\n",
        "mse_sgd = mean_squared_error(y, y_pred_sgd)\n",
        "mse_mini_batch = mean_squared_error(y, y_pred_mini_batch)\n",
        "\n",
        "# Imprimir resultados\n",
        "print(f\"Error MSE (Batch): {mse_batch:.4f}\")\n",
        "print(f\"Error MSE (SGD): {mse_sgd:.4f}\")\n",
        "print(f\"Error MSE (Mini-Batch): {mse_mini_batch:.4f}\")\n"
      ],
      "metadata": {
        "id": "h_0lT7dE_6Kg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}